<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.16.4 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Network members - Interpreting Deep Learning</title>
<meta name="description" content="Website for 2019 NWA-ORC proposal BD.1910: ‘Interpreting Deep Learning Models for Text and Sound: Methods &amp; Applications’.">



<meta property="og:type" content="website">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Interpreting Deep Learning">
<meta property="og:title" content="Network members">
<meta property="og:url" content="/members">




  <meta property="og:image" content="/assets/images/network-bw-1.png">









  

  


<link rel="canonical" href="/members">







  <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "Person",
      "name": "InterpretingDL",
      "url": "https://github.com/pages/interpretingdl/interpretingDL.github.io",
      "sameAs": null
    }
  </script>







<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Interpreting Deep Learning Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if IE ]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->
<link rel="apple-touch-icon" sizes="180x180" href="/assets/images/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/assets/images/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/assets/images/favicon-16x16.png">
<link rel="manifest" href="/assets/images/site.webmanifest">
<link rel="mask-icon" href="/assets/images/safari-pinned-tab.svg" color="#5bbad5">
<link rel="shortcut icon" href="/assets/images/favicon.ico">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="msapplication-config" content="/assets/images/browserconfig.xml">
<meta name="theme-color" content="#ffffff">

<!-- end custom head snippets -->

  </head>

  <body class="layout--single wide">

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/assets/images/brain.png" alt=""></a>
        
        <a class="site-title" href="/">InterpretingDL</a>
        <ul class="visible-links"></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      
  











<div class="page__hero--overlay"
  style="background-color: #5e616c; background-image: url('/assets/images/network-bw-1.png');"
>
  
    <div class="wrapper">
      <h1 id="page-title" class="page__title" itemprop="headline">
        
          Network members

        
      </h1>
      
      
      
      
    </div>
  
  
</div>





<div id="main" role="main">
  
  <div class="sidebar sticky">
  
  
    
      
      
      
      
    
    
      

<nav class="nav__list">
  
  <input id="ac-toc" name="accordion-toc" type="checkbox" />
  <label for="ac-toc">Toggle Menu</label>
  <ul class="nav__items">
    
      <li>
        
          
          

          <a href="/"><span class="nav__sub-title">Summary</span></a>
        

        
      </li>
    
      <li>
        
          
          

          <a href="/methods"><span class="nav__sub-title">Methods</span></a>
        

        
      </li>
    
      <li>
        
          
          

          <a href="/members"><span class="nav__sub-title">Members</span></a>
        

        
      </li>
    
      <li>
        
          
          

          <a href="/papers"><span class="nav__sub-title">Key papers</span></a>
        

        
      </li>
    
  </ul>
</nav>
    
  
  </div>


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Network members">
    
    
    

    <div class="page__inner-wrap">
      

      <section class="page__content" itemprop="text">
        
        <p>All members of the network are actively working on interpretability, but they have come to this topic from very different domains. The network brings together crucial expertise on methodology, lexical semantics, semantic and syntactic parsing, machine translation, computational phonology, music recommendation, language acquisition and more. This will allow us to give different perspectives towards what interpretation of deep learning means in different scenerios and for different goals.</p>

<p><img style="float: left; width: 20%; margin-right: 20px; margin-top: 15px; margin-bottom: 5px;" src="../assets/images/jelle2015.jpg" />
<strong>Willem Zuidema</strong> is associate professor of computational linguistics and cognitive science at ILLC (UvA), with a long term interest in the neural basis of language. Because of that cognitive interest, was early contributor to deep learning in NLP, with work on neural parsing published as early as 2008 (Borensztajn &amp; Zuidema, 2008, CogSci), and pioneering contributions on tree-shaped neural networks, including the TreeLSTM (Le &amp; Zuidema <a class="citation" href="#le2015">(2015)</a> <!--2015-->, *SEM; published concurrently with groups from Stanford and Montreal). In 2016 he and his students introduced Diagnostic Classification <a class="citation" href="#veldhoen2016">(Veldhoen, Hupkes, &amp; Zuidema, 2016; Hupkes, Veldhoen, &amp; Zuidema, 2018; Giulianelli, Harding, Mohnert, Hupkes, &amp; Zuidema, 2018)</a> <!--(Veldhoen et al., 2016; Hupkes et al 2018; Giulianelli et al. 2018)-->, one of the key <em>interpretability</em> techniques. He further performed research on the integration of formal logic and deep learning <a class="citation" href="#veldhoen2017">(Veldhoen &amp; Zuidema, 2017; Repplinger, Beinborn, &amp; Zuidema, 2018; Mul &amp; Zuidema, 2019)</a> <!--(Veldhoen & Zuidema, 2017; Repplinger, Beinborn & Zuidema, 2018; Mul & Zuidema, 2019)-->. Other directly relevant work focuses on other <em>interpretability techniques</em> including Representational Similarity Analysis <a class="citation" href="#abnar2019">(Abnar, Beinborn, Choenni, &amp; Zuidema, 2019)</a> <!--(Abnar et al., 2019)--> and contextual decomposition (Jumelet et al., 2019).</p>

<p><img style="float: left; width: 20%; margin-right: 20px; margin-top: 15px; margin-bottom: 5px;" src="../assets/images/alishahi.png" />
<strong>Afra Alishahi</strong> is an Associate Professor of Cognitive Science and Artificial Intelligence at Tilburg University. Her main research interests are developing computational models for studying the process of human language acquisition, studying the emergence of linguistic structure in grounded models of language learning, and developing tools and techniques for analyzing linguistic representations in neural models of language. She has received a number of research grants including an NWO Aspasia, an NWO Natural Artificial Intelligence and an e-Science Center/NWO grant. She is the co-organizer of the BlackboxNLP 2018 workshop, the first official venue dedicated to analyzing and interpreting neural networks for NLP. She has a number of well-received publications on the topic of interpretability of neural network models of language, including the recipient of the best paper award at the Conference on Computational Language Learning (CoNLL) in 2017.</p>

<p><img style="float: left; width: 20%; margin-right: 20px; margin-top: 15px; margin-bottom: 5px;" src="../assets/images/chrupała.jpg" />
<strong>Grzegorz Chrupała</strong> is an assistant professor at the Department of Cognitive Science and Artificial Intelligence at Tilburg University. His research focuses on computational models of language learning from multimodal signals such as speech and vision and on the analysis and interpretability of representations emerging in deep neural networks. He has served as area chair for ACL, EMNLP and CoNLL, and was general chair for Benelearn 2018. He co-organized the 2018 and 2019 editions of BlackboxNLP, the Workshop on Analyzing and Interpreting Neural Networks for NLP. Together with Afra Alishahi and students, he did some of the pioneering research on analyzing deep learning methods for visually grounded language <a class="citation" href="#kadar2017">(Kádár, Chrupała, &amp; Alishahi, 2017)</a><!--(Kádár, Chrupała and Alishahi 2017, CL)--> as well as for speech <a class="citation" href="#alishahi2017">(Alishahi, Barking, &amp; Chrupała, 2017)</a> <!--(Alishahi, Barking and Chrupała 2017, CoNLL)-->. In their most recent work in the area of analysis and interpretation Chrupała and Alishahi <a class="citation" href="#chrupala2019">(2019)</a> <!--(2019, ACL)--> introduced methods based on Representational Similarity Analysis (RSA) and Tree Kernels (TK) which directly quantify how strongly information encoded in neural activation patterns corresponds to information represented by symbolic structures.</p>

<p><img style="float: left; width: 20%; margin-right: 20px; margin-top: 10px; margin-bottom: 5px;" src="../assets/images/bisazza.jpg" />
<strong>Arianna Bisazza</strong>
is an assistant professor at the Leiden Institute of Advanced Computer Science (LIACS) of Leiden University, fully funded by a VENI grant since 2016. Her research aims at identifying intrinsic limitations of current language modeling paradigms, and to design robust NLP algorithms that can adapt to a diverse range of linguistic phenomena observed among the world’s languages. She has a long track record of contributions to machine translation for challenging language pairs <a class="citation" href="#bisazza2012">(Bisazza &amp; Federico, 2012; Tran, Bisazza, &amp; Monz, 2014; Fadaee, Bisazza, &amp; Monz, 2017)</a>
<!--(Bisazza & Federico 2012; Tran, Bisazza & Monz, 2014; Fadaee, Bisazza & Monz, 2016)-->. Together with colleagues at the University of Amsterdam, she proposed the Recurrent Memory Network, one of the very first modifications to deep-learning based language models aimed at improving interpretability <a class="citation" href="#tran2016">(Tran, Bisazza, &amp; Monz, 2016)</a><!--(Tran, Bisazza & Monz, 2016)-->. Other recent contributions to the interpretability of NLP models include analyses of MT outputs <a class="citation" href="#bentivogli2018">(Bentivogli, Bisazza, Cettolo, &amp; Federico, 2018)</a><!--(Bentivogli et al., 2018)--> and probing tasks for recurrent language models <a class="citation" href="#tran2018">(Tran, Bisazza, &amp; Monz, 2018; Bisazza &amp; Tump, 2018)</a><!--(Tran, Bisazza, and Monz, 2018; Bisazza & Tump, 2018)-->.</p>

<p><strong>Tom Lentz</strong> is an assistant professor in computational phonology and cognitive science at the ILLC of the UvA.  He works on the detection of prosodic structure in speech, including automatic classification of pitch contours as gathered in controlled experiments. He has recently obtained an interdisciplinary research grant for a project on the detection of irony in spoken speech (funding for one PhD student). Relevant other experience is an investigation on the individual variation in the use of prosody to mark focus <a class="citation" href="#lentz2015">(Lentz &amp; Chen, 2015)</a><!--(Lentz & Chen, 2015)-->.</p>

<p><strong>Louis ten Bosch</strong> (RU, Nijmegen) has expertise in automatic speech recognition, <em>computational modelling of cognitive processes</em>, speech decoding techniques using phonological features, and structure discovery methods. He is one of the coorganizers of the successful DNN interpretation session “what we learn from DNNs” that took place in 2018 at the language and speech technology conference Interspeech in Hyderabad, India. One of the recent advances in understanding artificial networks is achieved by relating the mathematical layer-to-layer transformations in a network to the more structural description of datasets as shown by linear mixed effect models and by Generalized Additive Models. More recently, in collaboration with Mirjam Ernestus, he is involved in computational models of human spoken word comprehension, a number of abstract-versus-exemplar studies in psycholinguistics, and (with Ton Dijkstra) in computational modelling of  online sentence processing of idiomatic expressions.</p>

<p><img style="float: left; width: 20%; margin-right: 20px; margin-top: 15px; margin-bottom: 5px;" src="../assets/images/hendrickx.jpeg" />
<strong>Iris Hendrickx</strong> (RU, Nijmegen) is a researcher in computational linguistics and digital humanities with a focus on the areas of machine learning, lexical and relational semantics, natural language processing, techniques for document understanding and text mining. She provides expertise to the network on creating text data enriched with human annotation for training such models, and on applying and evaluating these models and augmenting them with domain expert knowledge.</p>

<p><strong>Antske Fokkens</strong>
<!--is an assistant professor in computational linguistics at the Vrije Universiteit. Her main expertise lie in methodological questions in computational linguistics and, in particular, the importance of understanding the implications of chosen technologies, training data and features when applying computational language models in interdisciplinary contexts. She is a recognized international expert (with 16 funded international invitations) and has obtained multiple research grants, including a VENI grant in 2015 and co-applicantship of an NWO Vrije Competitie grant, as well as project funding from societal partners (leading up to a total of over 2.5M of research funds).--></p>

<p><strong>Ashley Burgoyne</strong>
<!--is the Lecturer in Computational Musicology at the University of Amsterdam and a member of the Music Cognition Group at the ILLC. With a background and cross-appointments in musicology and artificial intelligence, Burgoyne is interested in understanding musical behaviour at the audio level, using large-scale experiments and audio corpora. His McGill–Billboard corpus of time-aligned chord and structure transcriptions has served as a backbone for audio chord estimation techniques. His Hooked on Music project reached hundreds of thousands of participants in almost every country on Earth while collecting data to understand long-term musical memory. That data set will form the backbone of the musical subpackage in this grant.--></p>

<h2 id="references">References</h2>

<div class="bibliography"><div>




<div id="bib-item-abnar2019" class="bib-entry my-3" data-searchable="" data-year="2019" data-title="Blackbox meets blackbox: Representational Similarity and Stability Analysis of Neural Language Models and Brains" data-author="Abnar, Samira and Beinborn, Lisa and Choenni, Rochelle and Zuidema, Willem" data-publication="">
  <span id="abnar2019">Abnar, S., Beinborn, L., Choenni, R., &amp; Zuidema, W. (2019). Blackbox meets blackbox: Representational Similarity and Stability Analysis of Neural Language Models and Brains.</span><br />

    
    
      <a href="https://arxiv.org/abs/1906.01539" target="_blank">
        <button type="button" class="btn btn--inverse">
          arXiv
        </button></a>
    
</div>
<br />
</div>
<div>




<div id="bib-item-alishahi2017" class="bib-entry my-3" data-searchable="" data-year="2017" data-title="Encoding of phonology in a recurrent neural model of grounded speech" data-author="Alishahi, Afra and Barking, Marie and Chrupała, Grzegorz" data-publication="Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017)">
  <span id="alishahi2017">Alishahi, A., Barking, M., &amp; Chrupała, G. (2017). Encoding of phonology in a recurrent neural model of grounded speech. In R. Levy &amp; L. Specia (Eds.), <i>Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017)</i> (pp. 368–378). Association for Computational Linguistics.</span><br />

    
      <a href="https://doi.org/10.18653/v1/K17-1037" target="_blank" title="Encoding of phonology in a recurrent neural model of grounded speech">
        <button type="button" class="btn btn--inverse">
          DOI
        </button></a>
    
    
</div>
<br />
</div>
<div>




<div id="bib-item-bentivogli2018" class="bib-entry my-3" data-searchable="" data-year="2018" data-title="Neural versus phrase-based MT quality: An in-depth analysis on English–German and English–French" data-author="Bentivogli, Luisa and Bisazza, Arianna and Cettolo, Mauro and Federico, Marcello" data-publication="Computer Speech &amp; Language">
  <span id="bentivogli2018">Bentivogli, L., Bisazza, A., Cettolo, M., &amp; Federico, M. (2018). Neural versus phrase-based MT quality: An in-depth analysis on English–German and English–French. <i>Computer Speech &amp; Language</i>, <i>49</i>, 52–70.</span><br />

    
      <a href="https://doi.org/10.1016/j.csl.2017.11.004" target="_blank" title="Neural versus phrase-based MT quality: An in-depth analysis on English–German and English–French">
        <button type="button" class="btn btn--inverse">
          DOI
        </button></a>
    
    
</div>
<br />
</div>
<div>




<div id="bib-item-bisazza2012" class="bib-entry my-3" data-searchable="" data-year="2012" data-title="Cutting the Long Tail: Hybrid Language Models for Translation Style Adaptation" data-author="Bisazza, Arianna and Federico, Marcello" data-publication="Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics">
  <span id="bisazza2012">Bisazza, A., &amp; Federico, M. (2012). Cutting the Long Tail: Hybrid Language Models for Translation Style Adaptation. In <i>Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics</i> (pp. 439–448). Avignon, France: Association for Computational Linguistics.</span><br />

    
    
</div>
<br />
</div>
<div>




<div id="bib-item-bisazza2018" class="bib-entry my-3" data-searchable="" data-year="2018" data-title="The Lazy Encoder: A Fine-Grained Analysis of the Role of Morphology in Neural Machine Translation" data-author="Bisazza, Arianna and Tump, Clara" data-publication="Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing">
  <span id="bisazza2018">Bisazza, A., &amp; Tump, C. (2018). The Lazy Encoder: A Fine-Grained Analysis of the Role of Morphology in Neural Machine Translation. In <i>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</i> (pp. 2871–2876). Brussels, Belgium: Association for Computational Linguistics.</span><br />

    
    
</div>
<br />
</div>
<div>




<div id="bib-item-chrupala2019" class="bib-entry my-3" data-searchable="" data-year="2019" data-title="Correlating neural and symbolic representations of language" data-author="Chrupała, Grzegorz and Alishahi, Afra" data-publication="Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics">
  <span id="chrupala2019">Chrupała, G., &amp; Alishahi, A. (2019). Correlating neural and symbolic representations of language. In <i>Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics</i>.</span><br />

    
    
      <a href="https://arxiv.org/abs/1905.06401" target="_blank">
        <button type="button" class="btn btn--inverse">
          arXiv
        </button></a>
    
</div>
<br />
</div>
<div>




<div id="bib-item-fadaee2017" class="bib-entry my-3" data-searchable="" data-year="2017" data-title="Data Augmentation for Low-Resource Neural Machine Translation" data-author="Fadaee, Marzieh and Bisazza, Arianna and Monz, Christof" data-publication="Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)">
  <span id="fadaee2017">Fadaee, M., Bisazza, A., &amp; Monz, C. (2017). Data Augmentation for Low-Resource Neural Machine Translation. <i>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</i>, 567–573.</span><br />

    
      <a href="https://doi.org/10.18653/v1/P17-2090" target="_blank" title="Data Augmentation for Low-Resource Neural Machine Translation">
        <button type="button" class="btn btn--inverse">
          DOI
        </button></a>
    
    
      <a href="https://arxiv.org/abs/1705.00440" target="_blank">
        <button type="button" class="btn btn--inverse">
          arXiv
        </button></a>
    
</div>
<br />
</div>
<div>




<div id="bib-item-giulianelli2018" class="bib-entry my-3" data-searchable="" data-year="2018" data-title="Under the Hood: Using Diagnostic Classifiers to Investigate and Improve how Language Models Track Agreement Information" data-author="Giulianelli, Mario and Harding, Jack and Mohnert, Florian and Hupkes, Dieuwke and Zuidema, Willem" data-publication="Proceedings EMNLP workshop Analyzing and interpreting neural networks for NLP (BlackboxNLP)">
  <span id="giulianelli2018">Giulianelli, M., Harding, J., Mohnert, F., Hupkes, D., &amp; Zuidema, W. (2018). Under the Hood: Using Diagnostic Classifiers to Investigate and Improve how Language Models Track Agreement Information. In <i>Proceedings EMNLP workshop Analyzing and interpreting neural networks for NLP (BlackboxNLP)</i>.</span><br />

    
    
      <a href="https://arxiv.org/abs/1808.08079" target="_blank">
        <button type="button" class="btn btn--inverse">
          arXiv
        </button></a>
    
</div>
<br />
</div>
<div>




<div id="bib-item-hupkes2018" class="bib-entry my-3" data-searchable="" data-year="2018" data-title="Visualisation and ‘Diagnostic Classifiers’ reveal how recurrent and recursive neural networks process hierarchical structure" data-author="Hupkes, Dieuwke and Veldhoen, Sara and Zuidema, Willem" data-publication="Journal of Artificial Intelligence Research">
  <span id="hupkes2018">Hupkes, D., Veldhoen, S., &amp; Zuidema, W. (2018). Visualisation and ‘Diagnostic Classifiers’ reveal how recurrent and recursive neural networks process hierarchical structure. <i>Journal of Artificial Intelligence Research</i>, <i>61</i>, 907–926.</span><br />

    
    
      <a href="https://arxiv.org/abs/1711.10203" target="_blank">
        <button type="button" class="btn btn--inverse">
          arXiv
        </button></a>
    
</div>
<br />
</div>
<div>




<div id="bib-item-kadar2017" class="bib-entry my-3" data-searchable="" data-year="2017" data-title="Representation of Linguistic Form and Function in Recurrent Neural Networks" data-author="Kádár, Ákos and Chrupała, Grzegorz and Alishahi, Afra" data-publication="Computational Linguistics">
  <span id="kadar2017">Kádár, Á., Chrupała, G., &amp; Alishahi, A. (2017). Representation of Linguistic Form and Function in Recurrent Neural Networks. <i>Computational Linguistics</i>, <i>43</i>, 761–780.</span><br />

    
      <a href="https://doi.org/10.1162/COLI_a_00300" target="_blank" title="Representation of Linguistic Form and Function in Recurrent Neural Networks">
        <button type="button" class="btn btn--inverse">
          DOI
        </button></a>
    
    
</div>
<br />
</div>
<div>




<div id="bib-item-le2015" class="bib-entry my-3" data-searchable="" data-year="2015" data-title="Compositional Distributional Semantics with Long Short Term Memory" data-author="Le, Phong and Zuidema, Willem" data-publication="Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics">
  <span id="le2015">Le, P., &amp; Zuidema, W. (2015). Compositional Distributional Semantics with Long Short Term Memory. In <i>Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics</i> (pp. 10–19). Denver, Colorado: Association for Computational Linguistics.</span><br />

    
      <a href="https://doi.org/10.18653/v1/S15-1002" target="_blank" title="Compositional Distributional Semantics with Long Short Term Memory">
        <button type="button" class="btn btn--inverse">
          DOI
        </button></a>
    
    
</div>
<br />
</div>
<div>




<div id="bib-item-lentz2015" class="bib-entry my-3" data-searchable="" data-year="2015" data-title="Unbalanced adult production and perception in prosody." data-author="Lentz, T.O. and Chen, A." data-publication="Proceedings of the 18th International Congress of Phonetic Sciences">
  <span id="lentz2015">Lentz, T. O., &amp; Chen, A. (2015). Unbalanced adult production and perception in prosody. In <i>Proceedings of the 18th International Congress of Phonetic Sciences</i>. University of Glasgow, Glasgow.</span><br />

    
    
</div>
<br />
</div>
<div>




<div id="bib-item-mul2019" class="bib-entry my-3" data-searchable="" data-year="2019" data-title="Siamese recurrent networks learn first-order logic reasoning and exhibit zero-shot compositional generalization" data-author="Mul, Mathijs and Zuidema, Willem" data-publication="">
  <span id="mul2019">Mul, M., &amp; Zuidema, W. (2019). Siamese recurrent networks learn first-order logic reasoning and exhibit zero-shot compositional generalization.</span><br />

    
    
      <a href="https://arxiv.org/abs/1906.00180" target="_blank">
        <button type="button" class="btn btn--inverse">
          arXiv
        </button></a>
    
</div>
<br />
</div>
<div>




<div id="bib-item-repplinger2018" class="bib-entry my-3" data-searchable="" data-year="2018" data-title="Vector-space models of words and sentences" data-author="Repplinger, Michael and Beinborn, Lisa and Zuidema, Willem" data-publication="Nieuw Archief voor de Wiskunde">
  <span id="repplinger2018">Repplinger, M., Beinborn, L., &amp; Zuidema, W. (2018). Vector-space models of words and sentences. <i>Nieuw Archief Voor De Wiskunde</i>.</span><br />

    
    
</div>
<br />
</div>
<div>




<div id="bib-item-tran2016" class="bib-entry my-3" data-searchable="" data-year="2016" data-title="Recurrent Memory Networks for Language Modeling" data-author="Tran, Ke and Bisazza, Arianna and Monz, Christof" data-publication="Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies">
  <span id="tran2016">Tran, K., Bisazza, A., &amp; Monz, C. (2016). Recurrent Memory Networks for Language Modeling. In <i>Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</i> (pp. 321–331).</span><br />

    
      <a href="https://doi.org/10.18653/v1/N16-1036" target="_blank" title="Recurrent Memory Networks for Language Modeling">
        <button type="button" class="btn btn--inverse">
          DOI
        </button></a>
    
    
</div>
<br />
</div>
<div>




<div id="bib-item-tran2018" class="bib-entry my-3" data-searchable="" data-year="2018" data-title="The Importance of Being Recurrent for Modeling Hierarchical Structure" data-author="Tran, Ke and Bisazza, Arianna and Monz, Christof" data-publication="Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing">
  <span id="tran2018">Tran, K., Bisazza, A., &amp; Monz, C. (2018). The Importance of Being Recurrent for Modeling Hierarchical Structure. In <i>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</i> (pp. 4731–4736).</span><br />

    
    
      <a href="https://arxiv.org/abs/1803.03585" target="_blank">
        <button type="button" class="btn btn--inverse">
          arXiv
        </button></a>
    
</div>
<br />
</div>
<div>




<div id="bib-item-tran2014" class="bib-entry my-3" data-searchable="" data-year="2014" data-title="Word Translation Prediction for Morphologically Rich Languages with Bilingual Neural Networks" data-author="Tran, Ke M. and Bisazza, Arianna and Monz, Christof" data-publication="Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)">
  <span id="tran2014">Tran, K. M., Bisazza, A., &amp; Monz, C. (2014). Word Translation Prediction for Morphologically Rich Languages with Bilingual Neural Networks. In <i>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</i> (pp. 1676–1688). Association for Computational Linguistics.</span><br />

    
      <a href="https://doi.org/10.3115/v1/D14-1175" target="_blank" title="Word Translation Prediction for Morphologically Rich Languages with Bilingual Neural Networks">
        <button type="button" class="btn btn--inverse">
          DOI
        </button></a>
    
    
</div>
<br />
</div>
<div>




<div id="bib-item-veldhoen2016" class="bib-entry my-3" data-searchable="" data-year="2016" data-title="Diagnostic classifiers: revealing how neural networks process hierarchical structure" data-author="Veldhoen, Sara and Hupkes, Dieuwke and Zuidema, Willem" data-publication="Workshop on Cognitive Computation: Integrating Neural and Symbolic Approaches (at NIPS)">
  <span id="veldhoen2016">Veldhoen, S., Hupkes, D., &amp; Zuidema, W. (2016). Diagnostic classifiers: revealing how neural networks process hierarchical structure. In <i>Workshop on Cognitive Computation: Integrating Neural and Symbolic Approaches (at NIPS)</i>.</span><br />

    
    
</div>
<br />
</div>
<div>




<div id="bib-item-veldhoen2017" class="bib-entry my-3" data-searchable="" data-year="2017" data-title="Can Neural Networks learn Logical Reasoning?" data-author="Veldhoen, Sara and Zuidema, Willem" data-publication="Proceedings of the Conference on Logic and Machine Learning in Natural Language (LaML)">
  <span id="veldhoen2017">Veldhoen, S., &amp; Zuidema, W. (2017). Can Neural Networks learn Logical Reasoning? In <i>Proceedings of the Conference on Logic and Machine Learning in Natural Language (LaML)</i> (pp. pp. 35–41). University of Gothenburgh, Sweden.</span><br />

    
    
</div>
<br />
</div></div>

        
      </section>

      <footer class="page__meta">
        
        


        
      </footer>

      

      
    </div>

    
  </article>

  
  
</div>

    </div>

    

    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->

        <div class="page__footer-follow">
  <ul class="social-icons">
    

    

    <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2019 InterpretingDL. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>
  <script defer src="https://use.fontawesome.com/releases/v5.8.2/js/all.js" integrity="sha384-DJ25uNYET2XCl5ZF++U8eNxPWqcKohUUBUpKGlNLMchM7q4Wjg2CUpjHLaL8yYPH" crossorigin="anonymous"></script>










  </body>
</html>
